import numpy as npfrom scipy import linalgfrom collections import defaultdictimport nltkfrom nltk.tokenize import word_tokenize# func to return context words and center worddef get_windows(words, C):    index = C    while index < len(words) - C:        contextWords = words[index-C:index] + words[index+1:index+C+1]        centerWord = words[index]        yield contextWords, centerWord        index+=1# function to get the dictionary...def get_dict(data):    """    Input:        K: the number of negative samples        data: the data you want to pull from        indices: a list of word indices    Output:        word_dict: a dictionary with the weighted probabilities of each word        word2Ind: returns dictionary mapping the word to its index        Ind2Word: returns dictionary mapping the index to its word    """    # create a sorted list of vocabulary....    sortedList = sorted(list(set(data)))    # get total words...    totalWords = len(sortedList)    index = 0    word2index = {}    index2word = {}    for word in sortedList:        word2index[word] = index        index2word[index] = word        index = index + 1    return word2index, index2word# corpus = "I am happy because I am learning"## for x,y in get_windows(list(word_tokenize(corpus)),2):#     print(f'{x}\t{y}')# tokens = word_tokenize(corpus)## word2index, index2word = get_dict(tokens)# print("Word to Index is : ")# print(word2index)# print()# print("Index to word is : ")# print(index2word)